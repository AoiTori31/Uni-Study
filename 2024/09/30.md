# Paper reading 候補
* Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs

    https://arxiv.org/pdf/2401.06209

    Is vision good enough for language? 

    image-text 변환은 아직 정확도가 떨어짐

* Rich Human Feedback for Text-to-Image Generation

    https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf

    T2I모델들의 결함, Rich Human Feedback (RichHF-18K) 데이터셋과 Rich Automatic Human Feedback (RAHF) 모델 제안

   리뷰 

    https://velog.io/@bluein/paper-26



* Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation

    https://openaccess.thecvf.com/content/CVPR2024/papers/Ngo_Learning_CNN_on_ViT_A_Hybrid_Model_to_Explicitly_Class-specific_CVPR_2024_paper.pdf

    hybrid approach that combines CNN and ViT to leverage the strengths of both architectures